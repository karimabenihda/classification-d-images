**Classification dâ€™Images avec un RÃ©seau Neuronal**

***Construire et entrainer un rÃ©seau neuronal dense minimaliste (DNN) sur une dataset (images 28Ã—28, 10 classes) .Pour chargement et normalisation des images, construction dâ€™un modÃ¨le Keras simple, entrainement et Ã©valuation.*** 

**Lien trello:https://trello.com/b/ReHmBE1Q/classification-dimages**

**ğŸ§© Ã‰tape 1 :Importation des modÃ¨les et chargement du dataset**
***Charger Fashion MNIST via Tensorflow.keras.datasets.***

***tÃ©lÃ©charger data et tester les shapes du train & test***

***afficher les articles avec matplotlib***

***Visualiser la RÃ©partition des classes dans le dataset d'entraÃ®nement pour savoire si le dataset est equilibrer ou non car si on a la dominance d'une classes sur les autre classes le model va focucer sur celle qui est dominante***

***diffÃ©rence entre train loss et val loss:***
    ***Train loss â†’ il mesure Ã  quel point ton modÃ¨le se trompe sur les images quâ€™il connaÃ®t dÃ©jÃ .***
    ***Val loss â†’ il mesure Ã  quel point il se trompe sur de nouvelles images quâ€™il nâ€™a jamais vues.***

***ğŸ§  CrÃ©ation du modÃ¨le (Sequential):***
***Tu construis un rÃ©seau de neurones simple (DNN) avec Keras.Ce modÃ¨le contient 3 couches :***

***1.Flatten(input_shape=(28, 28))***
***->Transforme lâ€™image 2D (28x28 pixels) en un vecteur 1D (784 valeurs).***
***->Câ€™est comme â€œdÃ©roulerâ€ lâ€™image pour pouvoir lâ€™envoyer dans le rÃ©seau.***

***2.Dense(128, activation='relu')***
***->Couche entiÃ¨rement connectÃ©e (128 neurones).***
***->Fonction dâ€™activation ReLU : garde les valeurs positives, met les nÃ©gatives Ã  0 â†’ aide Ã  apprendre les motifs non linÃ©aires.***

***3.Dense(10, activation='softmax')***
***->Couche de sortie avec 10 neurones (parce quâ€™il y a 10 classes Ã  prÃ©dire, par exemple les chiffres de 0 Ã  9).***
***->Softmax transforme les sorties en probabilitÃ©s (la somme = 1).***

***âš™ï¸ Compilation du modÃ¨le***

***Tu dÃ©finis comment le modÃ¨le va apprendre :***

***optimizer='adam' â†’ Algorithme dâ€™optimisation rapide et efficace.***

***loss='sparse_categorical_crossentropy' â†’ Fonction de perte adaptÃ©e Ã  une classification multi-classes avec labels entiers (0â€“9).***

***metrics=['accuracy'] â†’ Pour suivre la prÃ©cision pendant lâ€™entraÃ®nement.***

***ğŸ§  EarlyStopping:***
***Tu utilises EarlyStopping pour arrÃªter automatiquement lâ€™entraÃ®nement quand le modÃ¨le ne sâ€™amÃ©liore plus.***

***ğŸ§  EntraÃ®nement du modÃ¨le: ***
***->X_train, y_train: DonnÃ©es dâ€™entraÃ®nement.***
***epochs=20: Nombre maximal de cycles dâ€™apprentissage.***
***->validation_data=(X_test, y_test): DonnÃ©es utilisÃ©es pour Ã©valuer la performance du modÃ¨le Ã  chaque Ã©poque.***
***->callbacks=[early_stop]: Utilise le mÃ©canisme EarlyStopping pour arrÃªter lâ€™entraÃ®nement si la performance cesse de sâ€™amÃ©liorer.***


***ğŸ§  Visualisation des performances du modÃ¨le:***
***-> 1er subplot(Train accuracy vs Validation accuracy):	Montrer comment la prÃ©cision du modÃ¨le Ã©volue Ã  chaque Ã©poque.***

***-> 2e subplot(Train loss vs Validation loss):	Montrer comment la perte diminue (ou augmente) pendant lâ€™apprentissage.***

**ğŸ§© Ã‰tape 2 : RÃ©diger le rapport (README.md)**



